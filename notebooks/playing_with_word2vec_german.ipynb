{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "source": [
      "\n",
      "One of hostest things in NLP/Deep Learning is [Word2Vec](https://code.google.com/p/word2vec/). Released as an open source project, Word2Vec is an Neural Netowrk Language model, developed mainly by Tomas Mikolov. It basically creates meaningful vector representation of words. Each dimension is somehow a similarity dimension which captures both syntactic and semantic information of a word. Check http://www.thisplusthat.me/ for an example usage and [this presentation](http://slid.es/christophermoody/thisplusthat) for a nice explanation.\n",
      "\n",
      "There are basically two ways we can use access word2vec vector from python one is using the [word2vec wraper](https://pypi.python.org/pypi/word2vec) that [Daniel Rodriguez](http://danielfrg.github.io/)  developed. The other way is to use it through [http://radimrehurek.com/gensim/models/word2vec.html](Genisim) by Radim Rehurek. \n",
      "\n",
      "I am chosing Gensim because it is a native reimplementation in python and offer nice functionality already.\n",
      "\n",
      "As I mentioned I am interested in the behavior of the word representations with the German language so I traine word2vec using the $3E^9$ bytes of the a German wikipedia dump. To train with the Wikipedia, we have to get the XML dumps and \"clean\" it from tags. To do that I adapted the script found at the end of [this page](http://mattmahoney.net/dc/textdata.html) to German. Basically replacing German \"funky\" characters.  I uploaded the adapted version as a [Gist](https://gist.github.com/mfcabrera/7674065).\n",
      "\n",
      "As for the training paramters for this particular test I used the skip-gram model and called word2vec like this:\n",
      "\n",
      "<pre>  time word2vec -train dewiki3e9.txt -output de3E9.bin -skipgram 5 -size 600 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1 -save-vocab defull-vocab.txt </pre>\n",
      " \n",
      "To test the model and write part of this blog I used [iPython Notebooks](http://ipython.org/notebook.html) and embeded the HTML output in my blog system.\n",
      "So, stop talking and let's start coding:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's get Gensim. I am assuming you have succesfully installed it\n",
      "\n",
      "from gensim.models import word2vec\n",
      "model = word2vec.Word2Vec.load_word2vec_format('../wordvecs/de3E9.bin',binary=True)\n"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "source": [
      "This takes time for this particular file. The vector file is almost 1GB and it has to be loaded in memory.  \n",
      "Once loaded the mdoel we can  some of  experiments found in the the paper and see how this particular model performs.  \n",
      "One of the cool example if that you can take the vector representing  _'king'_ add the vector of _'woman'_ and substract the vector of  _'man'_ and you will get vector which cosine distance is most similar to the vector representing _'queen'_. Let's see if that is true for this model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar(positive=['koenig', 'frau'], negative=['mann'])"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar(positive=['koenig', 'weib'], negative=['mann'])\n"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[('ferrantes', 0.55362153),\n",
        " ('koenigs', 0.55051303),\n",
        " ('kreon', 0.54674691),\n",
        " ('alberichs', 0.53421003),\n",
        " ('drosselbart', 0.53314435),\n",
        " ('tirian', 0.53156573),\n",
        " ('vladislavs', 0.52949393),\n",
        " ('ahabs', 0.52878058),\n",
        " ('eheversprechen', 0.52363348),\n",
        " ('herrscherpaar', 0.52357674)]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "source": [
      "Well it does not. But it does not surprise me. We do not have all the data available and the training paramters were chosen arbritarilly so no surprise that it does not work. However We got the word _'gemahlin'_ which is normally useful to refer to the wife of a King (consort). The word  _'gattin'_ is also used for spouse.  However we do see the word _'koenigin'_ and _'koenigsgemahlin'_ which is the  translation for _'queen'_ and 'royal consort'.  Let's see whats happen if I just add the words "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar(positive=['frau', 'koenig'], negative=['mann'])"
     ],
     "language": "python",
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'model' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-0dea8cc2d939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frau'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'koenig'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mann'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "source": [
      "<img src=\"http://upload.wikimedia.org/wikipedia/commons/7/71/Wei%C3%9Fwurst-1.jpg\" alt=\"Drawing\" style=\"width: 200px; text-align: center; float:right; margin:2em; \" />\n",
      "\n",
      "Wow well, almost :) - Only adding _'frau'_ to _'koenig'_ gave me in the top positions both _'queen'_ and _'consort'_.\n",
      "\n",
      "\n",
      "As I live in Munich, we often go on fridays to have a [_Weissw\u00fcrstfr\u00fchstuck_](http://en.wikipedia.org/wiki/Weisswurst) or a trational M\u00fcncher/Bayerisch breakfast. It is basically White sausace, sweet mustard and pretzel (accompained with an optional _Wiessbier_ or wheat beer). \n",
      " Let see if our Word2Vec model can differentiate  the components of this delicious meal. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"weisswuerst suessersenf\".split()\n",
      "model.doesnt_match(\"weisswuerst suessersenf brezn eierkuchen\".split())"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "'eierkuchen'"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "source": [
      "This actually worked pretty well. The model was able to caputure that ein eirkuchen is not part a traditional breakfast.\n",
      "\n",
      "On the referenced papers on word2vec webpage they describe some task both semantic and syntactic. let's try some of those and see how it works:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "questions  = [[\"berlin\", \"deutschland\", \"london\", \"england\"]]\n",
      "\n",
      "for q in questions:\n",
      "    print q\n",
      "    #print model.most_similar(positive=[q[1],q[2]],negative=[q[0]])\n"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['berlin', 'deutschland', 'london', 'england']\n"
       ]
      }
     ],
     "prompt_number": 6
    }
   ]
  }
 ]
}