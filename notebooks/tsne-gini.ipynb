{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "source": [
      "Training with Word2Vec Gensim implementation and displaying embedding using t-SNE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import gensim,  logging \n",
      "from  word2vec_util.io import FileSetencesGenerator\n",
      "import sys\n",
      "\n",
      "\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "filename= 'gini_just_text-no-single.txt'\n",
      "\n",
      "sentences = FileSetencesGenerator(filename)\n",
      "\n",
      "model = gensim.models.Word2Vec(sentences, min_count=10, size=150, workers=10, window=10)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:51,070 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:51,071 : INFO : collecting all words and their counts\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:51,089 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:52,455 : INFO : collected 109745 word types from a corpus of 2050064 words and 3517 sentences\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:52,505 : INFO : total 13509 word types after removing those with count<10\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:52,506 : INFO : constructing a huffman tree from 13509 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:53,541 : INFO : built huffman tree with maximum node depth 18\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:53,547 : INFO : resetting layer weights\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:53,880 : INFO : training model with 10 workers on 13509 vocabulary and 150 features\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:57,296 : INFO : PROGRESS: at 2.55% words, alpha 0.02500, 13910 words/s\n",
        "2014-03-15 19:01:58,168 : INFO : reached the end of input; waiting to finish 20 outstanding jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-15 19:01:58,321 : INFO : PROGRESS: at 21.85% words, alpha 0.02500, 91815 words/s\n",
        "2014-03-15 19:02:01,703 : INFO : PROGRESS: at 30.69% words, alpha 0.02299, 73193 words/s\n",
        "2014-03-15 19:02:05,759 : INFO : PROGRESS: at 59.14% words, alpha 0.01733, 92885 words/s\n",
        "2014-03-15 19:02:06,760 : INFO : PROGRESS: at 85.35% words, alpha 0.00666, 123624 words/s\n",
        "2014-03-15 19:02:08,185 : INFO : PROGRESS: at 88.05% words, alpha 0.00879, 114833 words/s\n",
        "2014-03-15 19:02:08,448 : INFO : training on 1852677 words took 14.6s, 127182 words/s\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "source": [
      "Now let's use only the top 'restrict_vocab' words that are also larger than 3 (character length)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "restrict_vocab = 300\n",
      "ok_vocab = [ x for x in     sorted(model.vocab.iteritems(),\n",
      "                  key=lambda item: -item[1].count)[0:restrict_vocab] if len(x[0]) > 3]\n",
      "\n",
      "\n",
      "#print ok_vocab\n",
      "\n"
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "#from fasttsne import _TSNE \n",
      "from fasttsne import fast_tsne\n",
      "\n",
      "#get only a list of word and vector presentation\n",
      "\n",
      "wordvecs = np.asarray([ model.syn0[value[1].index] for value in ok_vocab ])\n",
      "\n",
      "perplexity = 30.\n",
      "theta = 0.5\n",
      "\n",
      "#wordvecs,perplexity=perplexity, theta=theta)\n",
      "\n",
      "#tsn  = fasttsne.fast_tsne(\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "cannot import name fast_tsne",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-29-9eb990853928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfasttsne\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_TSNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfasttsne\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfast_tsne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#get only a list of word and vector presentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: cannot import name fast_tsne"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'fast_tsne'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-28-770ef7bbec3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasttsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_tsne\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'fast_tsne'"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "source": [
      "Now let's use Barnes-Hut TSE from OSDF Cython wrapper."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "['_TSNE',\n",
        " '__builtins__',\n",
        " '__doc__',\n",
        " '__file__',\n",
        " '__name__',\n",
        " '__package__',\n",
        " '__test__',\n",
        " 'np']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "outputs": []
    }
   ]
  }
 ]
}